#!/usr/bin/env python3
import time
from collections import deque
import numpy as np
import cv2
import math
import sys
import threading
import pygame
from flask import Flask, Response, jsonify

pygame.mixer.init()
try:
    alert_sound = pygame.mixer.Sound("/home/theo/alert.wav")
    print("Loaded alert.wav")
except Exception as e:
    print(f"Error loading sound: {e}")
    alert_sound = None

try:
    from tflite_runtime.interpreter import Interpreter
except Exception:
    from tensorflow.lite.python.interpreter import Interpreter

MODEL_PATH = "/home/theo/4.tflite"
CAMERA_ID = 0
INPUT_SIZE = 192
NECK_THRESHOLD = 30
SMOOTHING_FRAMES = 5
BAD_DURATION_TO_ALERT = 1.2
INFER_THREADS = 2

EYE_AR_THRESH = 0.25
EYE_AR_CONSEC_FRAMES = 3
FACE_TOO_CLOSE_RATIO = 0.35
FACE_TOO_FAR_RATIO = 0.08

interpreter = Interpreter(MODEL_PATH, num_threads=INFER_THREADS)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_dtype = input_details[0]['dtype']
in_h = input_details[0]['shape'][1]
in_w = input_details[0]['shape'][2]

def neck_angle(a, b, c):
    v1 = a - b
    v2 = c - b
    denom = (np.linalg.norm(v1) * np.linalg.norm(v2))
    if denom == 0:
        return 0.0
    cosang = np.dot(v1, v2) / denom
    cosang = np.clip(cosang, -1.0, 1.0)
    angle = math.degrees(math.acos(cosang))
    return abs(angle - 180.0)

def estimate_face_size(nose, left_ear, right_ear, frame_width):
    if left_ear is not None and right_ear is not None:
        face_width = np.linalg.norm(left_ear - right_ear)
    else:
        face_width = frame_width * 0.15
    face_ratio = face_width / frame_width
    return face_ratio

angle_buf = deque(maxlen=SMOOTHING_FRAMES)
ear_buf = deque(maxlen=3)
last_bad_time = None
alert_playing = False

total_blinks = 0
last_blink_time = 0
eyes_closed_frames = 0

cap = cv2.VideoCapture(CAMERA_ID)
if not cap.isOpened():
    print("Cannot open camera")
    sys.exit(1)

cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
cap.set(cv2.CAP_PROP_FPS, 30)
cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

app = Flask(__name__)
outputFrame = None
lock = threading.Lock()

stats = {
    'posture_status': 'Good',
    'blink_rate': 0,
    'distance_status': 'OK',
    'eye_status': 'Open'
}

draw_counter = 0

def detect_posture():
    global outputFrame, last_bad_time, alert_playing
    global total_blinks, last_blink_time, eyes_closed_frames
    global stats, draw_counter
    
    frame_count = 0
    start_time = time.time()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            continue
        
        frame_count += 1
        
        h, w, _ = frame.shape

        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img, (in_w, in_h), interpolation=cv2.INTER_NEAREST)
        
        if input_dtype == np.float32:
            inp = np.expand_dims(img_resized.astype(np.float32), axis=0)
            inp = (inp - 127.5) / 127.5
        else:
            inp = np.expand_dims(img_resized.astype(input_dtype), axis=0)

        interpreter.set_tensor(input_details[0]['index'], inp)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])

        kpts = output_data[0][0] if output_data.ndim == 4 else output_data[0]

        def kp(i):
            y, x, score = kpts[i]
            return np.array([x * w, y * h]), score

        idx = {
            'nose': 0, 'le': 1, 're': 2,
            'lear': 3, 'rear': 4,
            'ls': 5, 'rs': 6,
            'lh': 11, 'rh': 12
        }
        
        nose, s0 = kp(idx['nose'])
        le, s_le = kp(idx['le'])
        re, s_re = kp(idx['re'])
        lear, s_lear = kp(idx['lear'])
        rear, s_rear = kp(idx['rear'])
        ls, s1 = kp(idx['ls'])
        rs, s2 = kp(idx['rs'])
        lh, s3 = kp(idx['lh'])
        rh, s4 = kp(idx['rh'])

        min_conf = 0.25
        key_scores = [s0, s1, s2, s3, s4]
        
        eye_status = "Open"
        if s_le > min_conf and s_re > min_conf:
            eye_distance = np.linalg.norm(le - re)
            nose_to_eyes = (np.linalg.norm(nose - le) + np.linalg.norm(nose - re)) / 2
            
            if eye_distance > 0 and nose_to_eyes > 0:
                ear_estimate = eye_distance / nose_to_eyes
                ear_buf.append(ear_estimate)
                avg_ear = np.mean(ear_buf)
                
                if avg_ear < 0.35:
                    eyes_closed_frames += 1
                    if eyes_closed_frames >= EYE_AR_CONSEC_FRAMES:
                        eye_status = "Closed"
                else:
                    if eyes_closed_frames >= EYE_AR_CONSEC_FRAMES:
                        total_blinks += 1
                        last_blink_time = time.time()
                    eyes_closed_frames = 0

        distance_status = "OK"
        if s0 > min_conf:
            left_ear_pos = lear if s_lear > min_conf else None
            right_ear_pos = rear if s_rear > min_conf else None
            face_ratio = estimate_face_size(nose, left_ear_pos, right_ear_pos, w)
            
            if face_ratio > FACE_TOO_CLOSE_RATIO:
                distance_status = "TOO CLOSE"
            elif face_ratio < FACE_TOO_FAR_RATIO:
                distance_status = "Too far"

        if sum(s > min_conf for s in key_scores) < 3:
            posture_status = "Unknown"
            smoothed = 0
        else:
            mid_shoulder = (ls + rs) / 2.0
            mid_hip = (lh + rh) / 2.0
            angle_val = neck_angle(mid_hip, mid_shoulder, nose)
            angle_buf.append(angle_val)
            smoothed = float(np.mean(angle_buf))
            
            if smoothed > NECK_THRESHOLD:
                posture_status = "Bad"
                
                if last_bad_time is None:
                    last_bad_time = time.time()
                elapsed = time.time() - last_bad_time
                if elapsed >= BAD_DURATION_TO_ALERT:
                    if not alert_playing and alert_sound:
                        alert_sound.play()
                        alert_playing = True
            else:
                posture_status = "Good"
                last_bad_time = None
                alert_playing = False

        elapsed_time = time.time() - start_time
        blink_rate = (total_blinks / elapsed_time) * 60 if elapsed_time > 0 else 0
        fps = frame_count / elapsed_time if elapsed_time > 0 else 0

        draw_counter += 1
        if draw_counter % 3 == 0:
            if posture_status == "Bad":
                cv2.putText(frame, f"BAD: {smoothed:.1f}deg", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            elif posture_status == "Good":
                cv2.putText(frame, f"GOOD: {smoothed:.1f}deg", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            else:
                cv2.putText(frame, "No person", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            y_offset = 70
            cv2.putText(frame, f"Eyes: {eye_status}", (10, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
            cv2.putText(frame, f"Blinks: {total_blinks} ({blink_rate:.1f}/min)", (10, y_offset+22),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
            cv2.putText(frame, f"Distance: {distance_status}", (10, y_offset+44),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
            
            cv2.putText(frame, f"FPS: {fps:.1f}", (w-100, 25),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            if distance_status == "TOO CLOSE":
                cv2.putText(frame, "Move back!", (w//2-70, 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            elif distance_status == "Too far":
                cv2.putText(frame, "Move closer", (w//2-70, 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            
            if s_le > min_conf and s_re > min_conf:
                cv2.line(frame, tuple(le.astype(int)), tuple(re.astype(int)), (255, 0, 255), 1)
            
            if posture_status != "Unknown":
                mid_shoulder = (ls + rs) / 2.0
                mid_hip = (lh + rh) / 2.0
                cv2.line(frame, tuple(mid_hip.astype(int)), tuple(mid_shoulder.astype(int)), (0, 255, 0), 1)
                cv2.line(frame, tuple(mid_shoulder.astype(int)), tuple(nose.astype(int)), (0, 255, 0), 1)

        stats.update({
            'posture_status': posture_status,
            'blink_rate': f"{blink_rate:.1f}",
            'distance_status': distance_status,
            'eye_status': eye_status
        })

        with lock:
            outputFrame = frame.copy()

def generate():
    global outputFrame, lock
    while True:
        with lock:
            if outputFrame is None:
                time.sleep(0.01)
                continue
            (flag, encodedImage) = cv2.imencode(".jpg", outputFrame, 
                                                [cv2.IMWRITE_JPEG_QUALITY, 60])
            if not flag:
                continue
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' +
               bytearray(encodedImage) + b'\r\n')

@app.route("/video_feed")
def video_feed():
    return Response(generate(),
                    mimetype="multipart/x-mixed-replace; boundary=frame")

@app.route("/stats")
def get_stats():
    return jsonify(stats)

@app.route("/")
def index():
    return """
    <html>
      <head>
        <title>Posture + Eye Tracking Monitor</title>
        <style>
          body {
            background-color: #1a1a1a;
            color: #ffffff;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
          }
          .container {
            max-width: 1200px;
            margin: 0 auto;
          }
          h1 {
            text-align: center;
            color: #00ff88;
            margin-bottom: 30px;
          }
          .content {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
          }
          .video-container {
            flex: 1;
            min-width: 640px;
          }
          .video-container img {
            width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 20px rgba(0,255,136,0.3);
          }
          .stats-panel {
            flex: 0 0 300px;
            background: #2a2a2a;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.5);
          }
          .stat-item {
            padding: 15px;
            margin: 10px 0;
            background: #333;
            border-radius: 5px;
            border-left: 4px solid #00ff88;
          }
          .stat-label {
            font-size: 14px;
            color: #aaa;
            margin-bottom: 5px;
          }
          .stat-value {
            font-size: 20px;
            font-weight: bold;
            color: #00ff88;
          }
          .info {
            margin-top: 20px;
            padding: 15px;
            background: #2a2a2a;
            border-radius: 5px;
            font-size: 14px;
            color: #aaa;
          }
        </style>
        <script>
          function updateStats() {
            fetch('/stats')
              .then(r => r.json())
              .then(data => {
                document.getElementById('posture').textContent = data.posture_status;
                document.getElementById('eyes').textContent = data.eye_status;
                document.getElementById('blinks').textContent = data.blink_rate;
                document.getElementById('distance').textContent = data.distance_status;
                
                document.getElementById('posture').style.color = 
                  data.posture_status === 'Good' ? '#00ff88' : '#ff4444';
                document.getElementById('distance').style.color = 
                  data.distance_status === 'OK' ? '#00ff88' : '#ff8800';
              });
          }
          setInterval(updateStats, 1000);
          window.onload = updateStats;
        </script>
      </head>
      <body>
        <div class="container">
          <h1>Posture + Eye Tracking Monitor</h1>
          <div class="content">
            <div class="video-container">
              <img src="/video_feed" />
            </div>
            <div class="stats-panel">
              <h2 style="margin-top:0; color:#00ff88;">Live Stats</h2>
              <div class="stat-item">
                <div class="stat-label">Posture Status</div>
                <div class="stat-value" id="posture">-</div>
              </div>
              <div class="stat-item">
                <div class="stat-label">Eye Status</div>
                <div class="stat-value" id="eyes">-</div>
              </div>
              <div class="stat-item">
                <div class="stat-label">Blink Rate (per min)</div>
                <div class="stat-value" id="blinks">-</div>
              </div>
              <div class="stat-item">
                <div class="stat-label">Screen Distance</div>
                <div class="stat-value" id="distance">-</div>
              </div>
              <div class="info">
                <strong>Tips:</strong><br>
                Normal blink rate: 15-20/min<br>
                Keep neck angle under 30 degrees<br>
                Maintain proper distance<br>
                Take breaks every 20 mins
              </div>
            </div>
          </div>
        </div>
      </body>
    </html>
    """

if __name__ == '__main__':
    print("Starting Posture + Eye Tracking Monitor...")
    print("Access at: http://localhost:5000")
    
    t = threading.Thread(target=detect_posture)
    t.daemon = True
    t.start()
    
    app.run(host="0.0.0.0", port=5000, debug=False, threaded=True, use_reloader=False)
