#!/usr/bin/env python3
import time
from collections import deque
import numpy as np
import cv2
import math
import sys
import threading
import pygame
from flask import Flask, Response

# ==== Khởi tạo âm thanh ====
pygame.mixer.init()
try:
    alert_sound = pygame.mixer.Sound("/home/theo/alert.wav")
    print("Loaded alert.wav")
except Exception as e:
    print(f"Error loading sound: {e}")
    alert_sound = None

# ==== Try tflite_runtime first, else tensorflow fallback ====
try:
    from tflite_runtime.interpreter import Interpreter
except Exception:
    from tensorflow.lite.python.interpreter import Interpreter

# ==== Config ====
MODEL_PATH = "/home/theo/4.tflite"
CAMERA_ID = 0
INPUT_SIZE = 192
NECK_THRESHOLD = 30
SMOOTHING_FRAMES = 5
BAD_DURATION_TO_ALERT = 1.2
INFER_THREADS = 4

# ==== TFLite ====
interpreter = Interpreter(MODEL_PATH, num_threads=INFER_THREADS)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_dtype = input_details[0]['dtype']
in_h = input_details[0]['shape'][1]
in_w = input_details[0]['shape'][2]

# ==== Helpers ====
def neck_angle(a, b, c):
    v1 = a - b
    v2 = c - b
    denom = (np.linalg.norm(v1) * np.linalg.norm(v2))
    if denom == 0:
        return 0.0
    cosang = np.dot(v1, v2) / denom
    cosang = np.clip(cosang, -1.0, 1.0)
    angle = math.degrees(math.acos(cosang))
    return abs(angle - 180.0)

# ==== Buffers ====
angle_buf = deque(maxlen=SMOOTHING_FRAMES)
last_bad_time = None
alert_playing = False

# ==== Camera ====
cap = cv2.VideoCapture(CAMERA_ID)
if not cap.isOpened():
    print("Cannot open camera")
    sys.exit(1)

# ==== Flask ====
app = Flask(__name__)
outputFrame = None
lock = threading.Lock()

def detect_posture():
    global outputFrame, last_bad_time, alert_playing
    while True:
        ret, frame = cap.read()
        if not ret:
            continue
        h, w, _ = frame.shape

        # prepare input
        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img, (in_w, in_h))
        if input_dtype == np.float32:
            inp = np.expand_dims(img_resized.astype(np.float32), axis=0)
            inp = (inp - 127.5) / 127.5
        else:
            inp = np.expand_dims(img_resized.astype(input_dtype), axis=0)

        interpreter.set_tensor(input_details[0]['index'], inp)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])

        kpts = output_data[0][0] if output_data.ndim == 4 else output_data[0]
        print("Confidences:", [round(k[2], 3) for k in kpts])

        # draw keypoints
        for (y, x, score) in kpts:
            if score > 0.2:
                cv2.circle(frame, (int(x * w), int(y * h)), 3, (0, 0, 255), -1)

        def kp(i):
            y, x, score = kpts[i]
            return np.array([x * w, y * h]), score

        idx = {'nose': 0, 'ls': 5, 'rs': 6, 'lh': 11, 'rh': 12}
        nose, s0 = kp(idx['nose'])
        ls, s1 = kp(idx['ls'])
        rs, s2 = kp(idx['rs'])
        lh, s3 = kp(idx['lh'])
        rh, s4 = kp(idx['rh'])

        min_conf = 0.2
        scores = [s0, s1, s2, s3, s4]

        if sum(s > min_conf for s in scores) < 3:
            display_text = "No person / low confidence"
        else:
            mid_shoulder = (ls + rs) / 2.0
            mid_hip = (lh + rh) / 2.0
            angle_val = neck_angle(mid_hip, mid_shoulder, nose)
            angle_buf.append(angle_val)
            smoothed = float(np.mean(angle_buf))
            display_text = f"neck: {smoothed:.1f} deg"

            if smoothed > NECK_THRESHOLD:
                if last_bad_time is None:
                    last_bad_time = time.time()
                elapsed = time.time() - last_bad_time
                if elapsed >= BAD_DURATION_TO_ALERT:
                    if not alert_playing and alert_sound:
                        alert_sound.play()
                        alert_playing = True
            else:
                last_bad_time = None
                alert_playing = False

        # overlay text
        cv2.putText(frame, display_text, (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # update global frame
        with lock:
            outputFrame = frame.copy()

def generate():
    global outputFrame, lock
    while True:
        with lock:
            if outputFrame is None:
                continue
            (flag, encodedImage) = cv2.imencode(".jpg", outputFrame)
            if not flag:
                continue
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' +
               bytearray(encodedImage) + b'\r\n')

@app.route("/video_feed")
def video_feed():
    return Response(generate(),
                    mimetype="multipart/x-mixed-replace; boundary=frame")

@app.route("/")
def index():
    return """
    <html>
      <head><title>Posture Monitor</title></head>
      <body style="background-color:#222;color:white;font-family:Arial;">
        <h2>Posture Camera Stream</h2>
        <img src="/video_feed" width="640" height="480" />
      </body>
    </html>
    """

if __name__ == '__main__':
    t = threading.Thread(target=detect_posture)
    t.daemon = True
    t.start()
    app.run(host="0.0.0.0", port=5000, debug=False, threaded=True, use_reloader=False)
